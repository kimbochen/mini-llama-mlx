{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676e491e-497e-49b9-81b4-6a332c193ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    bsz: int = 16\n",
    "    lr: float = 1e-4\n",
    "    n_steps: int = 1000\n",
    "    pad_token_id: int = 65535  # Max value of uint16\n",
    "\n",
    "cfg_t = TrainerConfig()\n",
    "cfg_m = LLaMAConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50982c1a-8cad-422c-9467-1bf56826c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import config_dataloader\n",
    "load_data = config_dataloader(cfg_t.bsz, cfg_m.seq_len, cfg_t.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b7bfc2-0d8b-418c-8c55-2ad4a696d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "\n",
    "mx.set_default_device(mx.gpu)\n",
    "\n",
    "data_iter = iter(load_data())\n",
    "inputs_, targets_ = next(data_iter)\n",
    "while mx.all(inputs_ != cfg_t.pad_token_id):\n",
    "    targets_ = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d6c7905-d6da-4165-9ee4-e311c5e471f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama import LLaMAConfig, LLaMA\n",
    "model = LLaMA(**asdict(cfg_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c76d2104-17b9-4fb9-9058-58495b351412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx import nn\n",
    "\n",
    "def forward(model, inputs, targets):\n",
    "    pad_mask = (inputs != cfg_t.pad_token_id)\n",
    "    logits = model(inputs * pad_mask)\n",
    "\n",
    "    logprobs = nn.losses.cross_entropy(logits, targets)\n",
    "    logprobs_m = logprobs * pad_mask\n",
    "    loss = logprobs_m.sum() / pad_mask.sum()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05599ceb-909b-411b-b2a5-7ea4870bb0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(10.5277, dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_grad = nn.value_and_grad(model, forward)\n",
    "loss, grad = loss_and_grad(model, inputs_, targets_)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56f3377-81f2-4c6a-8477-27bed3eb1a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(61.2031, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "optimizer = optim.AdamW(learning_rate=cfg_t.lr)\n",
    "state = [model.state, optimizer.state]\n",
    "\n",
    "@partial(mx.compile, inputs=state, outputs=state)\n",
    "def train_step(inputs, targets):\n",
    "    loss_and_grad = nn.value_and_grad(model, forward)\n",
    "    loss, grads = loss_and_grad(model, inputs, targets)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss\n",
    "\n",
    "loss = train_step(inputs_, targets_)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b69943cf-2466-42ec-ae74-80f41ff15238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(55.4943, dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step(inputs_, targets_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28df2601-ea1e-4fd2-afd3-b596e5cf58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from mlx import nn\n",
    "import mlx.core as mx\n",
    "import mlx.optimizers as optim\n",
    "from dataset import config_dataloader\n",
    "from llama import LLaMAConfig, LLaMA\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    bsz: int = 16\n",
    "    lr: float = 1e-4\n",
    "    n_steps: int = 1000\n",
    "    pad_token_id: int = 65535  # Max value of uint16\n",
    "\n",
    "cfg_t = TrainerConfig()\n",
    "cfg_m = LLaMAConfig()\n",
    "\n",
    "model = LLaMA(**asdict(cfg_m))\n",
    "optimizer = optim.AdamW(learning_rate=cfg_t.lr)\n",
    "load_data = config_dataloader(cfg_t.bsz, cfg_m.seq_len, cfg_t.pad_token_id)\n",
    "\n",
    "inputs_BT, targets_BT = next(load_data(cfg_t.n_steps))\n",
    "\n",
    "# One train forward pass: inputs_BT, cfg_t.pad_token_id, model, targets_BT\n",
    "# pad_mask_BT = (inputs_BT != cfg_t.pad_token_id)\n",
    "# logits_BTV = model(inputs_BT * pad_mask_BT)\n",
    "# logprobs_BT = nn.losses.cross_entropy(logits_BTV, targets_BT)\n",
    "# loss = (logprobs_BT * pad_mask_BT).sum() / pad_mask_BT.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d28d790-fa62-4685-8c44-d299d432a0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy(124)\n",
      "Dummy(0)\n"
     ]
    }
   ],
   "source": [
    "class Dummy:\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "    def __add__(self, rhs):\n",
    "        return Dummy(self.val + rhs.val)\n",
    "    def __repr__(self):\n",
    "        return f'Dummy({self.val})'\n",
    "\n",
    "def test_scope():\n",
    "    def inner(x):\n",
    "        return x + y\n",
    "    y = Dummy(39)\n",
    "    x_ = Dummy(85)\n",
    "    print(inner(x_))\n",
    "    y.val = -85\n",
    "    print(inner(x_))\n",
    "test_scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5df78a-e70a-435a-b709-fcaab7a8ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forward_pass(model, inputs_BT, targets_BT):\n",
    "    pad_mask_BT = (inputs_BT != cfg_t.pad_token_id)\n",
    "    logits_BTV = model(inputs_BT * pad_mask_BT)\n",
    "    logprobs_BT = nn.losses.cross_entropy(logits_BTV, targets_BT)\n",
    "    loss = (logprobs_BT * pad_mask_BT).sum() / pad_mask_BT.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25cbd9d-5b14-44e5-b93b-f681aae0cecf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[compile] Function arguments must be trees of arrays or constants (floats, ints, or strings), but received type mlx.optimizers.optimizers.AdamW.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mupdate(model, grads)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstate, optimizer\u001b[38;5;241m.\u001b[39mstate\n\u001b[0;32m----> 7\u001b[0m model_state, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_BT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_BT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mupdate(model_state)\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mupdate(opt_state)\n",
      "\u001b[0;31mValueError\u001b[0m: [compile] Function arguments must be trees of arrays or constants (floats, ints, or strings), but received type mlx.optimizers.optimizers.AdamW."
     ]
    }
   ],
   "source": [
    "@mx.compile\n",
    "def train_step(model, optimizer, inputs_BT, targets_BT):\n",
    "    loss_and_grad = nn.value_and_grad(model, train_forward_pass)\n",
    "    loss, grads = loss_and_grad(model, inputs_BT, targets_BT)\n",
    "    optimizer.update(model, grads)\n",
    "    return model.state, optimizer.state\n",
    "model_state, opt_state = train_step(model, optimizer, inputs_BT, targets_BT)\n",
    "model.update(model_state)\n",
    "optimizer.update(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27f6dcaa-e908-443a-bc40-a766b89749a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': array(0, dtype=uint64), 'learning_rate': array(0.0001, dtype=float32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd581470-2c4b-4a73-8e29-b116263c8a00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
