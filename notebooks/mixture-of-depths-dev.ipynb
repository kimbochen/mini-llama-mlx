{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa8c050-28eb-48f3-81c1-91bd8803e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "from mlx import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935906c2-d490-4286-bdd7-45aa378bb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(3985)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdc7fbf4-a1d5-48f2-875f-477a1267045c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.324384, 0.0728916, 0.692562, ..., 0.473036, 0.732917, 0.613625],\n",
       "        [0.937988, 0.821932, 0.353415, ..., 0.699138, 0.0826687, 0.684287],\n",
       "        [0.963293, 0.217973, 0.239089, ..., 0.435873, 0.525348, 0.027538],\n",
       "        [0.332283, 0.22946, 0.521798, ..., 0.418693, 0.172103, 0.0305814]],\n",
       "       [[0.380298, 0.500459, 0.931418, ..., 0.795432, 0.0933626, 0.835255],\n",
       "        [0.328979, 0.496543, 0.868076, ..., 0.533726, 0.293846, 0.770115],\n",
       "        [0.442788, 0.287145, 0.0224269, ..., 0.171359, 0.840324, 0.175392],\n",
       "        [0.408249, 0.0423786, 0.482122, ..., 0.301558, 0.276174, 0.602193]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 2, 4, 8  # batch_size, seq_len, d_embd\n",
    "x = mx.random.uniform(shape=[B, T, C])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9579ebcb-2d05-4e9e-87f9-e34aa272ad78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.557108, 0.0590587, 0.603644, ..., 0.288003, 0.565407, 0.752665],\n",
       "         [-0.286823, 0.1454, -0.0832467, ..., 0.714576, 0.233552, 0.925167],\n",
       "         [1.15273, 1.4296, -0.337541, ..., 0.774492, 1.09024, -0.284072],\n",
       "         [0.694623, 1.15832, 0.722264, ..., 0.151736, 0.672384, -0.699643]],\n",
       "        [[0.947543, 2.18298, -0.407171, ..., 0.649991, 1.21402, -0.0334666],\n",
       "         [0.0934433, -0.0856612, 0.686615, ..., 0.866722, 0.30294, 0.53751],\n",
       "         [0.693493, 0.788395, -0.268603, ..., 0.715195, 1.10677, 0.194276],\n",
       "         [0.0828968, 0.234539, 0.210647, ..., 0.543489, 0.562059, 0.0940933]]], dtype=float32),\n",
       " array(0.705366, dtype=float32))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MixtureOfDepths(nn.Module):\n",
    "    def __init__(self, block, capacity_factor, seq_len, n_embd):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.capacity = int(capacity_factor * seq_len)\n",
    "        self.router = nn.Linear(n_embd, 1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T = x.shape[:2]  # batch_size, seq_len\n",
    "\n",
    "        # Top k expert choice\n",
    "        r = self.router(x).squeeze(-1)\n",
    "        capacity = min(self.capacity, self.capacity_factor*T)\n",
    "        chosen_idx = mx.argpartition(-r, capacity, axis=1)[:, :capacity]\n",
    "\n",
    "        # Sorted top k to preserve token causality\n",
    "        # mx.sort does not support uint32?\n",
    "        chosen_idx = mx.sort(chosen_idx.astype(mx.float32), axis=1).astype(mx.uint32)\n",
    "\n",
    "        # Process chosen tokens\n",
    "        batch_idx = mx.arange(B)[:, None]\n",
    "        chosen_r = r[batch_idx, chosen_idx, None]\n",
    "        chosen_x = x[batch_idx, chosen_idx, :]\n",
    "        process_x = self.block(chosen_x)\n",
    "        x[batch_idx, chosen_idx, :] += chosen_r * process_x\n",
    "\n",
    "        # Auxiliary loss for training the router\n",
    "        r_nll = -nn.log_softmax(chosen_r[..., 0], axis=-1).mean()\n",
    "\n",
    "        return x, r_nll\n",
    "\n",
    "mod = MixtureOfDepths(nn.Linear(C, C), 0.5, T, C)\n",
    "mod(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d831ee58-ee93-4382-bc26-05c75616591e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 128)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlx.core.fast import scaled_dot_product_attention\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_embd, p_drop, **kwargs):\n",
    "        assert d_embd % n_head == 0\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.scale = (d_embd / n_head) ** 0.5\n",
    "\n",
    "        self.attn_proj = nn.Linear(d_embd, 3*d_embd, bias=False)\n",
    "        self.out_proj = nn.Linear(d_embd, d_embd, bias=False)\n",
    "        self.resid_drop = nn.Dropout(p_drop)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T = x.shape[:2]\n",
    "\n",
    "        qkv = self.attn_proj(x).split(3, axis=-1)  # B, T, d_embd\n",
    "        to_attn_weights = lambda z: z.reshape(B, T, self.n_head, -1).transpose(0, 2, 1, 3)\n",
    "        Q, K, V = map(to_attn_weights, qkv)  # B, n_head, T, d_head\n",
    "\n",
    "        # MLX SDPA does not support dropout?\n",
    "        causal_mask = nn.MultiHeadAttention.create_additive_causal_mask(T)\n",
    "        O = scaled_dot_product_attention(Q, K, V, scale=self.scale, mask=causal_mask)  # B, n_head, T, d_head\n",
    "        O = O.transpose(0, 2, 1, 3).reshape(B, T, -1)  # B, T, d_embd\n",
    "\n",
    "        output = self.resid_drop(self.out_proj(O))\n",
    "\n",
    "        return output\n",
    "\n",
    "attn = CausalSelfAttention(8, 128, 0.1)\n",
    "attn(mx.random.uniform(shape=[2, 4, 128])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af59162d-4d13-461b-a928-2cc601cb4cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_embd, p_drop, **kwargs):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(d_embd, 4*d_embd, bias=False)\n",
    "        self.down_proj = nn.Linear(4*d_embd, d_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nn.gelu(self.up_proj(x))\n",
    "        x = self.dropout(self.down_proj(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0057d629-043e-4f6e-ac3c-22bde2cf9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_embd, **kwargs):\n",
    "        super().__init__()\n",
    "        self.pre_norm = nn.LayerNorm(d_embd, bias=False)\n",
    "        self.self_attn = CausalSelfAttention(d_embd=d_embd, **kwargs)\n",
    "        self.post_norm = nn.LayerNorm(d_embd, bias=False)\n",
    "        self.ffn = FeedForwardNet(d_embd=d_embd, **kwargs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.self_attn(self.pre_norm(x)) + x\n",
    "        x = self.ffn(self.post_norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7385929-e43c-477f-ad0b-1d4f0d34880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_vocab, n_ctx, d_embd, p_drop, n_layers, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_embd = nn.Embedding(n_vocab, d_embd)\n",
    "        self.pos_embd = nn.Embedding(n_ctx, d_embd)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "        self.blocks = [\n",
    "            TransformerBlock(d_embd=d_embd, p_drop=p_drop, **kwargs)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_embd, bias=False)\n",
    "        self.lm_proj = nn.Linear(d_embd, n_vocab, bias=False)\n",
    "\n",
    "    def __call__(self, tok_idx):\n",
    "        T = tok_idx.shape[1]\n",
    "\n",
    "        tok_embd = self.tok_embd(tok_idx)\n",
    "        pos_embd = self.pos_embd(mx.arange(T))\n",
    "        x = self.dropout(tok_embd + pos_embd)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        logits = self.lm_proj(self.norm(x))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc40f09e-d79f-409c-9343-d998c65529a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(n_vocab=128, n_ctx=32, n_layers=4, d_embd=256, n_head=8, p_drop=0.1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    n_vocab: int\n",
    "    n_ctx: int\n",
    "    n_layers: int\n",
    "    d_embd: int\n",
    "    n_head: int\n",
    "    p_drop: float\n",
    "\n",
    "cfg = ModelConfig(128, 32, 4, 256, 8, 0.1)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8dc46bdd-ec1d-4feb-9408-877955782d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 32, 128)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(**asdict(cfg))\n",
    "model(mx.random.randint(0, cfg.n_vocab, shape=[2, cfg.n_ctx])).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
