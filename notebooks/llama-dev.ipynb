{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c09be3-50a2-4fbe-ac93-fb1f5bcc5526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    '''\n",
    "    Model configuration reference:\n",
    "    https://github.com/epfml/llm-baselines?tab=readme-ov-file#results-on-wikitext\n",
    "    '''\n",
    "    n_layers: int = 24\n",
    "    vocab_size: int = 32000  # LLaMA 2 tokenizer\n",
    "    d_embd: int = 768\n",
    "    n_heads: int = 12\n",
    "    seq_len: int = 512\n",
    "    rope_theta: float = 1e4\n",
    "    rope_scale: float = 1.0\n",
    "    ffn_mult: int = 256\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "cfg_m = ModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca9d625-839c-49c9-96c8-52ff7fa9430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_test(B=2):\n",
    "    mx.random.seed(3985)\n",
    "    x_ = mx.random.uniform(shape=[B, cfg_m.seq_len, cfg_m.d_embd])\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea0b824-41b7-4e2a-bba5-d3df45d0556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "from mlx import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d71dd9e-203f-429e-a9df-7be6dcbdcaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 512, 768), (512, 768))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlx.core.fast import scaled_dot_product_attention\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_embd, n_heads, rope_theta, rope_scale, **kwargs):\n",
    "        super().__init__()\n",
    "        assert d_embd % n_heads == 0\n",
    "        self.d_head = d_embd // n_heads\n",
    "\n",
    "        self.attn_proj = nn.Linear(d_embd, 3*d_embd, bias=False)\n",
    "        self.rope = nn.RoPE(self.d_head, base=rope_theta, scale=rope_scale)\n",
    "        self.scale = self.d_head ** -0.5\n",
    "        self.out_proj = nn.Linear(d_embd, d_embd, bias=False)\n",
    "\n",
    "    def __call__(self, x, mask):\n",
    "        bsz, seq_len, d_embd = x.shape\n",
    "\n",
    "        # [bsz, seq_len, d_embd] * 3\n",
    "        qkv = self.attn_proj(x).split(3, axis=-1)\n",
    "\n",
    "        # bsz, n_heads, seq_len, d_head\n",
    "        to_attn_heads = lambda z: z.reshape(bsz, seq_len, -1, self.d_head).transpose(0, 2, 1, 3)\n",
    "        Q, K, V = map(to_attn_heads, qkv)\n",
    "\n",
    "        # Apply rotary embeddings\n",
    "        Q, K = self.rope(Q), self.rope(K)\n",
    "\n",
    "        # bsz, n_head, seq_len, d_head\n",
    "        O = scaled_dot_product_attention(Q, K, V, scale=self.scale, mask=mask)\n",
    "\n",
    "        # bsz, seq_len, d_embd\n",
    "        output = self.out_proj(O.transpose(0, 2, 1, 3).reshape(bsz, seq_len, d_embd))\n",
    "\n",
    "        return output\n",
    "\n",
    "attn = SelfAttention(**asdict(cfg_m))\n",
    "attn(x_test(), mask=nn.MultiHeadAttention.create_additive_causal_mask(cfg_m.seq_len)).shape, (cfg_m.seq_len, cfg_m.d_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "453e4e0d-d163-4884-86be-d83e044e48a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 512, 768)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_embd, ffn_mult, **kwargs):\n",
    "        super().__init__()\n",
    "        hidden_dim = int((4 * d_embd) * 2 / 3)\n",
    "        hidden_dim = ffn_mult * ((hidden_dim + ffn_mult - 1) // ffn_mult)  # The next multiple of ffn_mult\n",
    "\n",
    "        self.gate_proj = nn.Linear(d_embd, hidden_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(d_embd, hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(hidden_dim, d_embd, bias=False)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = nn.silu(self.gate_proj(x)) * self.up_proj(x)  # SwiGLU\n",
    "        out = self.down_proj(h)\n",
    "        return out\n",
    "\n",
    "ffn = FeedForwardNet(**asdict(cfg_m))\n",
    "ffn(x_test()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2a54139-e866-45b4-a7b3-4023b52d3d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 512, 768)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_embd, norm_eps, **kwargs):\n",
    "        super().__init__()\n",
    "        self.pre_norm = nn.RMSNorm(d_embd, norm_eps)\n",
    "        self.attn = SelfAttention(d_embd=d_embd, **kwargs)\n",
    "        self.ffn_norm = nn.RMSNorm(d_embd, norm_eps)\n",
    "        self.ffn = FeedForwardNet(d_embd=d_embd, **kwargs)\n",
    "\n",
    "    def __call__(self, x, mask):\n",
    "        h = x + self.attn(self.pre_norm(x), mask)\n",
    "        out = h + self.ffn(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "layer = TransformerBlock(**asdict(cfg_m))\n",
    "layer(x_test(), mask=nn.MultiHeadAttention.create_additive_causal_mask(cfg_m.seq_len)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d7fe044-e056-4ff4-9601-f959ebd340c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 512, 32000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LLaMA(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers, d_embd, norm_eps, **kwargs):\n",
    "        super().__init__()\n",
    "        self.embd_toks = nn.Embedding(vocab_size, d_embd)\n",
    "        self.layers = [\n",
    "            TransformerBlock(d_embd=d_embd, norm_eps=norm_eps,**kwargs)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        self.out_norm = nn.RMSNorm(d_embd, norm_eps)\n",
    "        self.lm_head = nn.Linear(d_embd, vocab_size, bias=False)\n",
    "\n",
    "    def __call__(self, tok_idxs):\n",
    "        # bsz, seq_len, d_embd\n",
    "        h = self.embd_toks(tok_idxs)\n",
    "\n",
    "        # bsz, seq_len, d_embd\n",
    "        causal_mask = nn.MultiHeadAttention.create_additive_causal_mask(h.shape[1])\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, causal_mask)\n",
    "        h = self.out_norm(h)\n",
    "\n",
    "        # bsz, seq_len, vocab_size\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = LLaMA(**asdict(cfg_m))\n",
    "model(mx.random.randint(0, cfg.vocab_size, shape=[2, cfg.seq_len])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d339ad76-b4e6-4241-9eac-fb747a148528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLaMA(\n",
       "  (embd_toks): Embedding(32000, 768)\n",
       "  (layers.0): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.1): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.2): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.3): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.4): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.5): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.6): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.7): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.8): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.9): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.10): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.11): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.12): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.13): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.14): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.15): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.16): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.17): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.18): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.19): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.20): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.21): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.22): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layers.23): TransformerBlock(\n",
       "    (pre_norm): RMSNorm(768, eps=1e-05)\n",
       "    (attn): SelfAttention(\n",
       "      (attn_proj): Linear(input_dims=768, output_dims=2304, bias=False)\n",
       "      (rope): RoPE(64, traditional=False)\n",
       "      (out_proj): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "    )\n",
       "    (ffn_norm): RMSNorm(768, eps=1e-05)\n",
       "    (ffn): FeedForwardNet(\n",
       "      (gate_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (up_proj): Linear(input_dims=768, output_dims=2048, bias=False)\n",
       "      (down_proj): Linear(input_dims=2048, output_dims=768, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (out_norm): RMSNorm(768, eps=1e-05)\n",
       "  (lm_head): Linear(input_dims=768, output_dims=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
